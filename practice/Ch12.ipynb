{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "sys.path.insert(0, '/mnt/afml/ml_finance/mlfinlab')\n",
    "from mlfinlab.data_structures import imbalance_data_structures as imbar, standard_data_structures as bar\n",
    "import mlfinlab as ml\n",
    "\n",
    "sys.path.insert(0, '/mnt/afml/ml_finance/finance_ml')\n",
    "from finance_ml import sampling, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parq(fname):\n",
    "    table = pq.read_table(fname)\n",
    "    df = table.to_pandas()\n",
    "    df = df.set_index('TIMESTAMP')\n",
    "    ''' 중복된 index 제거, volume은 더해준다 '''\n",
    "    df = df.sort_values(by='TIMESTAMP')  # 중복 데이터 무시\n",
    "    df_v = df.groupby(df.index).sum()\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    df['V'] = df_v['V']\n",
    "    df['DV'] = df_v['DV']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dataset/TRADE_A233740_2018.parq'\n",
    "df = load_parq(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dollar bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dataset/TRADE_A233740_2018.csv'\n",
    "bar_fname = 'dataset/DBAR_A233740_2018.csv'\n",
    "if not os.path.exists(fname):\n",
    "    df_csv = df.reset_index()[['TIMESTAMP', 'PRICE', 'V']]\n",
    "    df_csv.columns = ['date_time', 'price', 'volume']\n",
    "    df_csv['price'] = df_csv['price'].astype('float')\n",
    "    df_csv.to_csv(fname, index=False)\n",
    "    \n",
    "if os.path.exists(bar_fname):\n",
    "    dbar = pd.read_csv(bar_fname, index_col='date_time')\n",
    "    dbar.index = pd.to_datetime(dbar.index)\n",
    "else:\n",
    "    dbar = bar.get_dollar_bars(fname, threshold=1e8)\n",
    "    dbar.index = pd.to_datetime(dbar.index)\n",
    "    dbar.to_csv(bar_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7647271, 5)\n",
      "(518545, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(dbar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply triple barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp('2018-01-03 09:00:21.481000')\n"
     ]
    }
   ],
   "source": [
    "# Compute daily volatility\n",
    "daily_vol = ml.util.get_daily_vol(close=dbar['close'], lookback=50)\n",
    "\n",
    "# Apply Symmetric CUSUM Filter and get timestamps for events\n",
    "# Note: Only the CUSUM filter needs a point estimate for volatility\n",
    "daily_vol_mean = daily_vol.rolling(10000).mean()\n",
    "cusum_events = ml.filters.cusum_filter(dbar['close'], daily_vol_mean=daily_vol_mean)\n",
    "\n",
    "# Compute vertical barrier\n",
    "vertical_barriers = ml.labeling.add_vertical_barrier(t_events=cusum_events, close=dbar['close'], num_days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary - Build Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-20 18:06:25.563012 100.0% apply_pt_sl_on_t1 done after 0.02 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "pt_sl = [1, 2]\n",
    "min_ret = 0.01\n",
    "triple_barrier_events = ml.labeling.get_events(close=dbar['close'],\n",
    "                                               t_events=cusum_events,\n",
    "                                               pt_sl=pt_sl,\n",
    "                                               target=daily_vol,\n",
    "                                               min_ret=min_ret,\n",
    "                                               num_threads=3,\n",
    "                                               vertical_barrier_times=vertical_barriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p = ml.labeling.get_bins(triple_barrier_events, dbar['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    334\n",
       " 1    283\n",
       " 0    136\n",
       "Name: bin, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_p.bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dbar.copy()\n",
    "\n",
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['close'].pct_change(periods=2)\n",
    "raw_data['mom5'] = raw_data['close'].pct_change(periods=5)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=50, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=15, center=False).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)\n",
    "\n",
    "# Remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features at event dates\n",
    "X = raw_data\n",
    "\n",
    "# Drop unwanted columns\n",
    "try:\n",
    "    X.drop(['open', 'high', 'low', 'close', 'volume'], axis=1, inplace=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y = labels_p['bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_idx = y.index.join(X.index).join(labels_p.index)\n",
    "X = X.loc[com_idx]\n",
    "y = y.loc[com_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of Backtest Overfitting (PBO)\n",
    "### Combinatorial Purged Cross Valdiation (CPCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_ml.model_selection import CPKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(5)\n",
    "for train_idx, test_idx in kfold.split(X, y):\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_train = X[train_idx]\n",
    "    X_train = X[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpcv = CPKFold((6,2), t1=triple_barrier_events['t1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 train: 500, test 252\n",
      "S2 train: 496, test 252\n",
      "S3 train: 498, test 251\n",
      "S4 train: 491, test 251\n",
      "S5 train: 500, test 251\n",
      "S6 train: 476, test 252\n",
      "S7 train: 473, test 251\n",
      "S8 train: 466, test 251\n",
      "S9 train: 475, test 251\n",
      "S10 train: 495, test 251\n",
      "S11 train: 488, test 251\n",
      "S12 train: 497, test 251\n",
      "S13 train: 496, test 250\n",
      "S14 train: 499, test 250\n",
      "S15 train: 500, test 250\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "for train_index, test_index in cpcv.split(X, y):\n",
    "    print(\"S{} train: {}, test {}\".format(idx, len(train_index), len(test_index)))\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 train: 0.55, test: 0.4166666666666667\n",
      "S2 train: 0.4899193548387097, test: 0.39285714285714285\n",
      "S3 train: 0.5020080321285141, test: 0.4342629482071713\n",
      "S4 train: 0.5112016293279023, test: 0.4262948207171315\n",
      "S5 train: 0.488, test: 0.4302788844621514\n",
      "S6 train: 0.5042016806722689, test: 0.38095238095238093\n",
      "S7 train: 0.5348837209302325, test: 0.4262948207171315\n",
      "S8 train: 0.5343347639484979, test: 0.4701195219123506\n",
      "S9 train: 0.5031578947368421, test: 0.398406374501992\n",
      "S10 train: 0.494949494949495, test: 0.40239043824701193\n",
      "S11 train: 0.5122950819672131, test: 0.38645418326693226\n",
      "S12 train: 0.5090543259557344, test: 0.3665338645418327\n",
      "S13 train: 0.5584677419354839, test: 0.408\n",
      "S14 train: 0.49899799599198397, test: 0.432\n",
      "S15 train: 0.552, test: 0.38\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "R_train = []\n",
    "R_test = []\n",
    "depth = 2\n",
    "n_estimator = 100\n",
    "for train_index, test_index in cpcv.split(X, y):\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator,\n",
    "                                criterion='entropy')\n",
    "    rf.fit(X_train, y_train.values.ravel())\n",
    "    R_train.append(rf.score(X_train, y_train))\n",
    "    R_test.append(rf.score(X_test, y_test))\n",
    "    print(\"S{} train: {}, test: {}\".format(idx, R_train[-1], R_test[-1]))\n",
    "    idx += 1\n",
    "print(\"train: {}, test: {}\".format(np.mean(R_train), np.mean(R_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.5162314478255252, test: 0.4101008031366597\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 train: 0.558, test: 0.4126984126984127\n",
      "S2 train: 0.5241935483870968, test: 0.38492063492063494\n",
      "S3 train: 0.5461847389558233, test: 0.398406374501992\n",
      "S4 train: 0.5498981670061099, test: 0.42231075697211157\n",
      "S5 train: 0.542, test: 0.44621513944223107\n",
      "S6 train: 0.5819327731092437, test: 0.35714285714285715\n",
      "S7 train: 0.5496828752642706, test: 0.4541832669322709\n",
      "S8 train: 0.5901287553648069, test: 0.4342629482071713\n",
      "S9 train: 0.5389473684210526, test: 0.41434262948207173\n",
      "S10 train: 0.5313131313131313, test: 0.3904382470119522\n",
      "S11 train: 0.569672131147541, test: 0.3784860557768924\n",
      "S12 train: 0.5553319919517102, test: 0.3665338645418327\n",
      "S13 train: 0.5604838709677419, test: 0.412\n",
      "S14 train: 0.5410821643286573, test: 0.408\n",
      "S15 train: 0.588, test: 0.384\n",
      "train: 0.555123434414479, test: 0.40426274584202876\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "R_train = []\n",
    "R_test = []\n",
    "depth = 3\n",
    "n_estimator = 10\n",
    "for train_index, test_index in cpcv.split(X, y):\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator,\n",
    "                                criterion='entropy')\n",
    "    rf.fit(X_train, y_train.values.ravel())\n",
    "    R_train.append(rf.score(X_train, y_train))\n",
    "    R_test.append(rf.score(X_test, y_test))\n",
    "    print(\"S{} train: {}, test: {}\".format(idx, R_train[-1], R_test[-1]))\n",
    "    idx += 1\n",
    "print(\"train: {}, test: {}\".format(np.mean(R_train), np.mean(R_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afml",
   "language": "python",
   "name": "afml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
