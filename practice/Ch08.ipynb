{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/afml/venv/lib/python3.5/site-packages/pyfolio/pos.py:28: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  ' to position notionals.'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pyfolio as pf\n",
    "\n",
    "sys.path.insert(0, '/mnt/afml/ml_finance/mlfinlab')\n",
    "from mlfinlab.data_structures import imbalance_data_structures as imbar, standard_data_structures as bar\n",
    "import mlfinlab as ml\n",
    "\n",
    "sys.path.insert(0, '/mnt/afml/ml_finance/finance_ml')\n",
    "from finance_ml import sampling, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parq(fname):\n",
    "    table = pq.read_table(fname)\n",
    "    df = table.to_pandas()\n",
    "    df = df.set_index('TIMESTAMP')\n",
    "    ''' 중복된 index 제거, volume은 더해준다 '''\n",
    "    df = df.sort_values(by='TIMESTAMP')  # 중복 데이터 무시\n",
    "    df_v = df.groupby(df.index).sum()\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    df['V'] = df_v['V']\n",
    "    df['DV'] = df_v['DV']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dataset/TRADE_A233740_2018.parq'\n",
    "df = load_parq(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dataset/TRADE_A233740_2018.csv'\n",
    "bar_fname = 'dataset/DBAR_A233740_2018.csv'\n",
    "if not os.path.exists(fname):\n",
    "    df_csv = df.reset_index()[['TIMESTAMP', 'PRICE', 'V']]\n",
    "    df_csv.columns = ['date_time', 'price', 'volume']\n",
    "    df_csv['price'] = df_csv['price'].astype('float')\n",
    "    df_csv.to_csv(fname, index=False)\n",
    "    \n",
    "if os.path.exists(bar_fname):\n",
    "    dbar = pd.read_csv(bar_fname, index_col='date_time')\n",
    "    dbar.index = pd.to_datetime(dbar.index)\n",
    "else:\n",
    "    dbar = bar.get_dollar_bars(fname, threshold=1e8)\n",
    "    dbar.index = pd.to_datetime(dbar.index)\n",
    "    dbar.to_csv(bar_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp('2018-01-03 09:00:21.481000')\n"
     ]
    }
   ],
   "source": [
    "# Compute daily volatility\n",
    "daily_vol = ml.util.get_daily_vol(close=dbar['close'], lookback=50)\n",
    "\n",
    "# Apply Symmetric CUSUM Filter and get timestamps for events\n",
    "# Note: Only the CUSUM filter needs a point estimate for volatility\n",
    "daily_vol_mean = daily_vol.rolling(10000).mean()\n",
    "cusum_events = ml.filters.cusum_filter(dbar['close'], daily_vol_mean=daily_vol_mean)\n",
    "\n",
    "# Compute vertical barrier\n",
    "vertical_barriers = ml.labeling.add_vertical_barrier(t_events=cusum_events, close=dbar['close'], num_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 16:12:17.435514 100.0% apply_pt_sl_on_t1 done after 0.1 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "pt_sl = [1, 1]\n",
    "min_ret = 0.005\n",
    "triple_barrier_events = ml.labeling.get_events(close=dbar['close'],\n",
    "                                               t_events=cusum_events,\n",
    "                                               pt_sl=pt_sl,\n",
    "                                               target=daily_vol,\n",
    "                                               min_ret=min_ret,\n",
    "                                               num_threads=3,\n",
    "                                               vertical_barrier_times=vertical_barriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p = ml.labeling.get_bins(triple_barrier_events, dbar['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dbar.copy()\n",
    "\n",
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['close'].pct_change(periods=2)\n",
    "raw_data['mom5'] = raw_data['close'].pct_change(periods=5)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=50, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=15, center=False).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)\n",
    "\n",
    "# Remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['open' 'high' 'low' 'close' 'volume'] not found in axis\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/afml/venv/lib/python3.5/site-packages/pandas/core/indexing.py:1017: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return getattr(section, self.name)[new_key]\n"
     ]
    }
   ],
   "source": [
    "# Get features at event dates\n",
    "X = raw_data\n",
    "\n",
    "# Drop unwanted columns\n",
    "try:\n",
    "    X.drop(['open', 'high', 'low', 'close', 'volume'], axis=1, inplace=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "y = labels_p.loc[X.index,'bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.dropna()\n",
    "X = X.dropna()\n",
    "com_idx = y.index.join(X.index).join(labels_p.index)\n",
    "X = X.loc[com_idx]\n",
    "y = y.loc[com_idx]\n",
    "labels_p = labels_p.loc[com_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_co_events = get_num_co_events(dbar['close'].index, t1, 2)\n",
    "sample_weight = get_sample_tw(t1, n_co_events, triple_barrier_events.index)\n",
    "labels_p['w'] = sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p['t1'] = triple_barrier_events['t1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from finance_ml.model_selection import PurgedKFold\n",
    "from finance_ml.model_selection import cv_score\n",
    "\n",
    "\n",
    "def feat_imp_MDI(forest, feat_names):\n",
    "    imp_dict = {i:tree.feature_importances_ for i, tree in enumerate(forest.estimators_)}\n",
    "    imp_df = pd.DataFrame.from_dict(imp_dict, orient='index')\n",
    "    imp_df.columns = feat_names\n",
    "    # 0 simply means not used for splitting\n",
    "    imp_df = imp_df.replace(0, np.nan)\n",
    "    imp = pd.concat({'mean': imp_df.mean(),\n",
    "                     'std': imp_df.std() * np.sqrt(imp_df.shape[0])},\n",
    "                    axis=1)\n",
    "    imp /= imp['mean'].sum()\n",
    "    return imp\n",
    "\n",
    "\n",
    "def feat_imp_MDA(clf, X, y, n_splits, sample_weight, t1, pct_embargo, scoring='neg_log_loss'):\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('wrong scoring method')\n",
    "    cv_gen = PurgedKFold(n_splits=n_splits, t1=t1, pct_embargo=pct_embargo)\n",
    "    index = np.arange(n_splits)\n",
    "    scores = pd.Series(index=index)\n",
    "    scores_perm = pd.DataFrame(index=index, columns=X.columns)\n",
    "    for idx, (train, test) in zip(index, cv_gen.split(X=X)):\n",
    "        X_train = X.iloc[train]\n",
    "        y_train = y.iloc[train]\n",
    "        w_train = sample_weight.iloc[train]\n",
    "        X_test = X.iloc[test]\n",
    "        y_test = y.iloc[test]\n",
    "        w_test = sample_weight.iloc[test]\n",
    "        clf_fit = clf.fit(X_train, y_train, sample_weight=w_train.values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = clf_fit.predict_proba(X_test)\n",
    "            scores.loc[idx] = -log_loss(y_test, prob, sample_weight=w_test.values,\n",
    "                                     labels=clf_fit.classes_)\n",
    "        else:\n",
    "            pred = clf_fit.predict(X_test)\n",
    "            scores.loc[idx] = accuracy_score(y_test,  pred, sample_weight=w_test.values)\n",
    "        \n",
    "        for col in X.columns:\n",
    "            X_test_ = X_test.copy(deep=True)\n",
    "            # Randomize certain feature to make it not effective\n",
    "            np.random.shuffle(X_test_[col].values)\n",
    "            if scoring == 'neg_log_loss':\n",
    "                prob = clf_fit.predict_proba(X_test_)\n",
    "                scores_perm.loc[idx, col] = -log_loss(y_test, prob, sample_weight=w_test.value,\n",
    "                                                    labels=clf_fit.classes_)\n",
    "            else:\n",
    "                pred = clf_fit.predict(X_test_)\n",
    "                scores_perm.loc[idx, col] = accuracy_score(y_test, pred, sample_weight=w_test.values)\n",
    "    # (Original score) - (premutated score)\n",
    "    imprv = (-scores_perm).add(scores, axis=0)\n",
    "    # Relative to maximum improvement\n",
    "    if scoring == 'neg_log_loss':\n",
    "        max_imprv = -scores_perm\n",
    "    else:\n",
    "        max_imprv = 1. - scores_perm\n",
    "    imp = imprv / max_imprv\n",
    "    imp = pd.DataFrame({'mean': imp.mean(), 'std': imp.std() * np.sqrt(imp.shape[0])})\n",
    "    return imp, scores.mean()\n",
    "    \n",
    "\n",
    "def aux_feat_imp_SFI(feat_names, clf, X, cont, scoring, cv_gen):\n",
    "    imp = pd.DataFrame(columns=['mean', 'std'])\n",
    "    for feat_name in feat_names:\n",
    "        scores = cv_score(clf, X=X[[feat_name]], y=cont['bin'],\n",
    "                          sample_weight=cont['w'],\n",
    "                          scoring=scoring,\n",
    "                          cv_gen=cv_gen)\n",
    "        imp.loc[feat_name, 'mean'] = scores.mean()\n",
    "        imp.loc[feat_name, 'std'] = scores.std() * np.sqrt(scores.shape[0])\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from finance_ml.multiprocessing import mp_pandas_obj\n",
    "from finance_ml.model_selection import PurgedKFold\n",
    "\n",
    "\n",
    "def feat_importance(X, cont, clf=None, n_estimators=1000, n_splits=10, max_samples=1.,\n",
    "                    num_threads=24, pct_embargo=0., scoring='accuracy',\n",
    "                    method='SFI', min_w_leaf=0., **kwargs):\n",
    "    n_jobs = (-1 if num_threads > 1 else 1)\n",
    "    # Build classifiers\n",
    "    if clf is None:\n",
    "        base_clf = DecisionTreeClassifier(criterion='entropy', max_features=1,\n",
    "                                          class_weight='balanced',\n",
    "                                          min_weight_fraction_leaf=min_w_leaf)\n",
    "        clf = BaggingClassifier(base_estimator=base_clf, n_estimators=n_estimators,\n",
    "                                max_features=1., max_samples=max_samples,\n",
    "                                oob_score=True, n_jobs=n_jobs)\n",
    "    fit_clf = clf.fit(X, cont['bin'], sample_weight=cont['w'].values)\n",
    "    if hasattr(fit_clf, 'oob_score_'):\n",
    "        oob = fit_clf.oob_score_\n",
    "    else:\n",
    "        oob = None\n",
    "    if method == 'MDI':\n",
    "        imp = feat_imp_MDI(fit_clf, feat_names=X.columns)\n",
    "        oos = cv_score(clf, X=X, y=cont['bin'], n_splits=n_splits,\n",
    "                       sample_weight=cont['w'], t1=cont['t1'],\n",
    "                       pct_embargo=pct_embargo, scoring=scoring).mean()\n",
    "    elif method == 'MDA':\n",
    "        imp, oos = feat_imp_MDA(clf, X=X, y=cont['bin'], n_splits=n_splits,\n",
    "                                sample_weight=cont['w'], t1=cont['t1'],\n",
    "                                pct_embargo=pct_embargo, scoring=scoring)\n",
    "    elif method == 'SFI':\n",
    "        cv_gen = PurgedKFold(n_splits=n_splits, t1=cont['t1'], pct_embargo=pct_embargo)\n",
    "        oos = cv_score(clf, X=X, y=cont['bin'], sample_weight=cont['w'],\n",
    "                       scoring=scoring, cv_gen=cv_gen)\n",
    "        clf.n_jobs = 1\n",
    "        imp = mp_pandas_obj(aux_feat_imp_SFI, ('feat_names', X.columns),\n",
    "                            num_threads, clf=clf, X=X, cont=cont,\n",
    "                            scoring=scoring, cv_gen=cv_gen)\n",
    "    return imp, oob, oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   mean       std\n",
      "log_ret        0.069718  0.126554\n",
      "mom1           0.071654  0.113598\n",
      "mom2           0.087857  0.131110\n",
      "mom5           0.095547  0.135671\n",
      "volatility_50  0.108679  0.133467\n",
      "0.49322643074371025\n",
      "0.4888130534628822\n",
      "CPU times: user 1min 21s, sys: 8 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(oob_score=True, n_estimators=100) \n",
    "imp_MDI, oob_MDI, oos_MDI = feat_importance(X, labels_p, clf=clf, method='MDI')\n",
    "print(imp_MDI.head())\n",
    "print(oob_MDI)\n",
    "print(oos_MDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   mean       std\n",
      "log_ret       -0.030267  0.233171\n",
      "mom1          -0.033689  0.199446\n",
      "mom2          -0.006696  0.197762\n",
      "mom5          -0.010387  0.164880\n",
      "volatility_50 -0.016944  0.090630\n",
      "0.49433231960188\n",
      "0.4642213883516435\n",
      "CPU times: user 1min 24s, sys: 40 ms, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = RandomForestClassifier(oob_score=True, n_estimators=100) \n",
    "imp_MDA, oob_MDA, oos_MDA = feat_importance(X, labels_p, clf=clf, method='MDA')\n",
    "print(imp_MDA.head())\n",
    "print(oob_MDA)\n",
    "print(oos_MDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-11 20:34:39.232385 100.0% aux_feat_imp_SFI done after 8.54 minutes. Remaining 0.0 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mean        std\n",
      "autocorr_1    0.4434  0.0644523\n",
      "autocorr_3  0.440873   0.113346\n",
      "log_ret     0.451602    0.11345\n",
      "log_t1      0.468481  0.0925326\n",
      "log_t2      0.455479     0.1251\n",
      "0.4946087918164225\n",
      "[0.48266697 0.42317597 0.47497846 0.46732354 0.49861526 0.44876843\n",
      " 0.47911657 0.51830764 0.42670918 0.46488995]\n",
      "CPU times: user 1min 25s, sys: 1.03 s, total: 1min 26s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = RandomForestClassifier(oob_score=True, n_estimators=100) \n",
    "imp_SFI, oob_SFI, oos_SFI = feat_importance(X, labels_p, clf=clf, method='SFI')\n",
    "print(imp_SFI.head())\n",
    "print(oob_SFI)\n",
    "print(oos_SFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_t1', 'mom5', 'volatility_50', 'log_t2', 'mom1', 'log_ret',\n",
       "       'volatility_15', 'mom2', 'log_t5', 'autocorr_1', 'autocorr_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_SFI.sort_values('mean', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['volatility_50', 'volatility_15', 'autocorr_1', 'autocorr_3', 'mom5',\n",
       "       'log_t5', 'log_t2', 'mom2', 'log_t1', 'mom1', 'log_ret'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_MDI.sort_values('mean', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_t5', 'log_t1', 'mom2', 'mom5', 'autocorr_1', 'volatility_50',\n",
       "       'log_ret', 'volatility_15', 'mom1', 'log_t2', 'autocorr_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_MDA.sort_values('mean', ascending=False).index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afml",
   "language": "python",
   "name": "afml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
