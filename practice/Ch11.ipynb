{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "sys.path.insert(0, '/mnt/afml/ml_finance/mlfinlab')\n",
    "from mlfinlab.data_structures import imbalance_data_structures as imbar, standard_data_structures as bar\n",
    "import mlfinlab as ml\n",
    "\n",
    "sys.path.insert(0, '/mnt/afml/ml_finance/finance_ml')\n",
    "from finance_ml import sampling, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parq(fname):\n",
    "    table = pq.read_table(fname)\n",
    "    df = table.to_pandas()\n",
    "    df = df.set_index('TIMESTAMP')\n",
    "    ''' 중복된 index 제거, volume은 더해준다 '''\n",
    "    df = df.sort_values(by='TIMESTAMP')  # 중복 데이터 무시\n",
    "    df_v = df.groupby(df.index).sum()\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    df['V'] = df_v['V']\n",
    "    df['DV'] = df_v['DV']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dataset/TRADE_A233740_2018.parq'\n",
    "df = load_parq(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dollar bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dataset/TRADE_A233740_2018.csv'\n",
    "bar_fname = 'dataset/DBAR_A233740_2018.csv'\n",
    "if not os.path.exists(fname):\n",
    "    df_csv = df.reset_index()[['TIMESTAMP', 'PRICE', 'V']]\n",
    "    df_csv.columns = ['date_time', 'price', 'volume']\n",
    "    df_csv['price'] = df_csv['price'].astype('float')\n",
    "    df_csv.to_csv(fname, index=False)\n",
    "    \n",
    "if os.path.exists(bar_fname):\n",
    "    dbar = pd.read_csv(bar_fname, index_col='date_time')\n",
    "    dbar.index = pd.to_datetime(dbar.index)\n",
    "else:\n",
    "    dbar = bar.get_dollar_bars(fname, threshold=1e8)\n",
    "    dbar.index = pd.to_datetime(dbar.index)\n",
    "    dbar.to_csv(bar_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(dbar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply triple barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily volatility\n",
    "daily_vol = ml.util.get_daily_vol(close=dbar['close'], lookback=50)\n",
    "\n",
    "# Apply Symmetric CUSUM Filter and get timestamps for events\n",
    "# Note: Only the CUSUM filter needs a point estimate for volatility\n",
    "daily_vol_mean = daily_vol.rolling(10000).mean()\n",
    "cusum_events = ml.filters.cusum_filter(dbar['close'], daily_vol_mean=daily_vol_mean)\n",
    "\n",
    "# Compute vertical barrier\n",
    "vertical_barriers = ml.labeling.add_vertical_barrier(t_events=cusum_events, close=dbar['close'], num_days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary - Build Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sl = [1, 2]\n",
    "min_ret = 0.01\n",
    "triple_barrier_events = ml.labeling.get_events(close=dbar['close'],\n",
    "                                               t_events=cusum_events,\n",
    "                                               pt_sl=pt_sl,\n",
    "                                               target=daily_vol,\n",
    "                                               min_ret=min_ret,\n",
    "                                               num_threads=3,\n",
    "                                               vertical_barrier_times=vertical_barriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p = ml.labeling.get_bins(triple_barrier_events, dbar['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p.bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dbar.copy()\n",
    "\n",
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['close'].pct_change(periods=2)\n",
    "raw_data['mom5'] = raw_data['close'].pct_change(periods=5)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=50, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=15, center=False).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)\n",
    "\n",
    "# Remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features at event dates\n",
    "X = raw_data\n",
    "\n",
    "# Drop unwanted columns\n",
    "try:\n",
    "    X.drop(['open', 'high', 'low', 'close', 'volume'], axis=1, inplace=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y = labels_p['bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_idx = y.index.join(X.index).join(labels_p.index)\n",
    "X = X.loc[com_idx]\n",
    "y = y.loc[com_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary - Fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split data as 1/4/7/10\n",
    "train  1/4 , 1/7 , 1/10, 4/7 , 4/10, 7/10\n",
    "test   7/10, 4/10, 4/7 , 1/10, 1/7 , 1/4\n",
    "\"\"\"\n",
    "train_idx_list = [[1,4], [1,7], [1,10], [4,7], [4,10], [7,10]]\n",
    "N = 30\n",
    "pbo = 0\n",
    "total_iter = 0\n",
    "n_estimator = 100\n",
    "depth = 2\n",
    "lc_list = []\n",
    "for train_idx in tqdm(train_idx_list):\n",
    "    R_train = []\n",
    "    R_test = []\n",
    "    test_idx = [x for x in [1,4,7,10] if x not in train_idx]\n",
    "    train_m1 = train_idx[0]\n",
    "    train_m2 = train_idx[1]\n",
    "    test_m1 = test_idx[0]\n",
    "    test_m2 = test_idx[1]\n",
    "    X_train = pd.concat([X.loc[datetime.date(2018, train_m1, 1): datetime.date(2018, train_m1+2, 28)],\n",
    "                         X.loc[datetime.date(2018, train_m2, 1): datetime.date(2018, train_m2+2, 28)]], axis=0)\n",
    "    y_train = pd.concat([y.loc[datetime.date(2018, train_m1, 1): datetime.date(2018, train_m1+2, 28)],\n",
    "                         y.loc[datetime.date(2018, train_m2, 1): datetime.date(2018, train_m2+2, 28)]], axis=0)\n",
    "    X_test = pd.concat([X.loc[datetime.date(2018, test_m1, 1): datetime.date(2018, test_m1+2, 28)],\n",
    "                        X.loc[datetime.date(2018, test_m2, 1): datetime.date(2018, test_m2+2, 28)]], axis=0)\n",
    "    y_test = pd.concat([y.loc[datetime.date(2018, test_m1, 1): datetime.date(2018, test_m1+2, 28)],\n",
    "                        y.loc[datetime.date(2018, test_m2, 1): datetime.date(2018, test_m2+2, 28)]], axis=0)\n",
    "    for _ in range(N):\n",
    "        rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator,\n",
    "                                    criterion='entropy')\n",
    "        rf.fit(X_train, y_train.values.ravel())\n",
    "        R_train.append(rf.score(X_train, y_train))\n",
    "        R_test.append(rf.score(X_test, y_test))\n",
    "    best_idx = np.argmax(R_train)\n",
    "    test_rank = rankdata(R_test, method='ordinal')\n",
    "    wc = test_rank[best_idx]/N\n",
    "    if wc == 0:\n",
    "        lc = -1e9\n",
    "    elif wc == 1:\n",
    "        lc = 1e9\n",
    "    else:\n",
    "        lc = np.log(wc/(1-wc))\n",
    "    lc_list.append(lc)\n",
    "m = np.mean(lc_list)\n",
    "s = np.std(lc_list)\n",
    "z = (0-m)/s\n",
    "pbo = norm.cdf(z)\n",
    "print(\"PBO:{:.2f}%%\".format(100*pbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "pbo = 0\n",
    "total_iter = 0\n",
    "n_estimator = 100\n",
    "depth = 2\n",
    "for train_idx in tqdm(itertools.combinations(list(range(1,13)), 6), total=924):\n",
    "    R_train = []\n",
    "    R_test = []\n",
    "    test_idx = [x for x in list(range(1,13)) if x not in train_idx]\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = pd.Series()\n",
    "    X_test = pd.DataFrame()\n",
    "    y_test = pd.Series()\n",
    "    for train_m in list(train_idx):\n",
    "        X_train = X_train.append(X.loc[datetime.date(2018,train_m,1):datetime.date(2018,train_m,28)])\n",
    "        y_train = y_train.append(y.loc[datetime.date(2018,train_m,1):datetime.date(2018,train_m,28)])\n",
    "    for test_m in list(test_idx):\n",
    "        X_test = X_test.append(X.loc[datetime.date(2018,test_m,1):datetime.date(2018,test_m,28)])\n",
    "        y_test = y_test.append(y.loc[datetime.date(2018,test_m,1):datetime.date(2018,test_m,28)])\n",
    "    for _ in range(N):\n",
    "        rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator,\n",
    "                                    criterion='entropy')\n",
    "        rf.fit(X_train, y_train.values.ravel())\n",
    "        R_train.append(rf.score(X_train, y_train))\n",
    "        R_test.append(rf.score(X_test, y_test))\n",
    "    best_idx = np.argmax(R_train)\n",
    "    test_rank = rankdata(R_test, method='ordinal')\n",
    "    wc = test_rank[best_idx]/N\n",
    "    if wc == 0:\n",
    "        lc = -1e9\n",
    "    elif wc == 1:\n",
    "        lc = 1e9\n",
    "    else:\n",
    "        lc = np.log(wc/(1-wc))\n",
    "    lc_list.append(lc)\n",
    "m = np.mean(lc_list)\n",
    "s = np.std(lc_list)\n",
    "z = (0-m)/s\n",
    "pbo = norm.cdf(z)\n",
    "print(\"PBO:{:.2f}%%\".format(100*pbo))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afml",
   "language": "python",
   "name": "afml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
